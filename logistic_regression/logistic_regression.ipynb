{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "def logistic_regression(X, y, learning_rate=0.01, epochs=1000, test_size=0.0):\n",
    "    if test_size != 0.0:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = X, X, y, y\n",
    "    \n",
    "    n_features = X_train.shape[1]\n",
    "    w = np.zeros(n_features)\n",
    "    b = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(len(X_train)):\n",
    "            z = np.dot(X_train[i], w) + b\n",
    "            a = 1 / (1 + np.exp(-z))\n",
    "            \n",
    "            \n",
    "            dw = (a - y_train[i]) * X_train[i]\n",
    "            db = a - y_train[i]\n",
    "            \n",
    "            \n",
    "            w -= learning_rate * dw\n",
    "            b -= learning_rate * db\n",
    "    \n",
    "    \n",
    "    correct = 0\n",
    "    for i in range(len(X_test)):\n",
    "        z = np.dot(X_test[i], w) + b\n",
    "        a = 1 / (1 + np.exp(-z))\n",
    "        prediction = 1 if a >= 0.5 else 0\n",
    "        if prediction == y_test[i]:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / len(X_test) if len(X_test) > 0 else 0\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ', '.', ',', '?', '!', ':', ';', '-', '_', '+', '=', '(', ')', '{', '}', '[', ']', '<', '>', '/', '\\\\', 'B', 'M', ' M', ' Ma', ' B', ' Mau', ' Mauv', ' Mauva', ' Mauvai', ' Mauvais', 'on', ' Bi', ' Bie', ' Bien', ' Mal', ' Bon', 'Bon', 'Bon Bien', 'Bon Bien Mal', 'Bon Bien Mal Mauvais', 'Bon Bien Mal Mauvais Mauvais', 'Bon Bien Mal Mauvais Mauvais Mal', 'Bon Bien Mal Mauvais Mauvais Mal Bien', 'Bon Bien Mal Mauvais Mauvais Mal Bien Bon', 'Bon Bien Mal Mauvais Mauvais Mal Bien Bon Mal', 'Bon Bien Mal Mauvais Mauvais Mal Bien Bon Mal Bon', 'Bon Bien Mal Mauvais Mauvais Mal Bien Bon Mal Bon Mauvais', 'Bon Bien Mal Mauvais Mauvais Mal Bien Bon Mal Bon Mauvais Bien', 'Bon Bien Mal Mauvais Mauvais Mal Bien Bon Mal Bon Mauvais Bien ', 'Bon Bien Mal Mauvais Mauvais Mal Bien Bon Mal Bon Mauvais Bien t', 'Bon Bien Mal Mauvais Mauvais Mal Bien Bon Mal Bon Mauvais Bien te', 'Bon Bien Mal Mauvais Mauvais Mal Bien Bon Mal Bon Mauvais Bien tes']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "module_path = os.path.abspath(os.path.join(current_dir, '..', 'tokenizer'))\n",
    "sys.path.append(module_path)\n",
    "\n",
    "from tokenizer import BPETokenizer\n",
    "bpe = BPETokenizer()\n",
    "bpe.bpe_train(\"Bon Bien Mal Mauvais Mauvais Mal Bien Bon Mal Bon Mauvais Bien test Mauvais\", 30)\n",
    "data = [\n",
    "    \"Bon\",\n",
    "    \" Bien\",\n",
    "    \" Mal\",\n",
    "    \" Mauvais\"\n",
    "]\n",
    "for i in range(len(data)):\n",
    "    data[i] = bpe.encode(data[i])\n",
    "print(bpe.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[63]\n",
      " [60]\n",
      " [61]\n",
      " [56]]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([i for i in data])\n",
    "print(data)\n",
    "labels = np.array([1, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(token_ids, vocab_size):\n",
    "    result = np.zeros((len(token_ids), vocab_size))\n",
    "    for i, sample in enumerate(token_ids):\n",
    "        for token_id in sample:\n",
    "            result[i, token_id] = 1\n",
    "    return result\n",
    "vocab_size = len(bpe.vocab)\n",
    "one_hot_data = one_hot_encode(data, vocab_size)\n",
    "w , b = logistic_regression(one_hot_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8983010198093685\n",
      "0.10161524298028801\n"
     ]
    }
   ],
   "source": [
    "# Test with \"Bon\"\n",
    "print(1/(1 + np.exp(-(np.dot(one_hot_data[0],w) + b))))\n",
    "# Test with \"Mauvais\"\n",
    "print(1/(1 + np.exp(-(np.dot(one_hot_data[3],w) + b))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
