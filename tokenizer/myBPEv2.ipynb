{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize(text):\n",
    "    \"\"\"\n",
    "    Pretokenize the input text by splitting it into words and punctuation.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    # Define a regex pattern to match words and punctuation\n",
    "    pattern = r\"\\w+|[^\\w\\s]\"\n",
    "    # Find all matches in the input text\n",
    "    tokens = re.findall(pattern, text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '!', 'This', 'is', 'a', 'test', '.', '.', '.', 'Isn', \"'\", 't', 'it', '?']\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Hello world! This is a test... Isn't it?\"\n",
    "pre_tokens = pretokenize(test_text)\n",
    "print(pre_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-tokens: ['This', 'is', 'the', 'first', 'sentence', '.', 'This', 'document', 'is', 'the', 'second', 'document', '.', 'And', 'this', 'is', 'the', 'third', 'one', '.', 'Is', 'this', 'the', 'first', 'document', '?']\n",
      "Initial Character Vocabulary: ['.', '</w>', '<unk>', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u']\n"
     ]
    }
   ],
   "source": [
    "# Sample Corpus \n",
    "corpus = [\n",
    "  \"This is the first sentence.\",\n",
    "  \"This document is the second document.\",\n",
    "  \"And this is the third one.\",\n",
    "  \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "all_pre_tokens = []\n",
    "for sentence in corpus:\n",
    "  all_pre_tokens.extend(pretokenize(sentence)) \n",
    "print(\"Pre-tokens:\", all_pre_tokens)\n",
    "\n",
    "initial_vocab = set()\n",
    "for token in all_pre_tokens:\n",
    "  initial_vocab.update(list(token))\n",
    "initial_vocab.update([\"<unk>\",\"</w>\"])\n",
    "\n",
    "print(\"Initial Character Vocabulary:\", sorted(list(initial_vocab)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Frequencies: Counter({'the': 4, 'is': 3, '.': 3, 'document': 3, 'This': 2, 'first': 2, 'this': 2, 'sentence': 1, 'second': 1, 'And': 1, 'third': 1, 'one': 1, 'Is': 1, '?': 1})\n",
      "Initial Splits: {'This': ['T', 'h', 'i', 's', '</w>'], 'is': ['i', 's', '</w>'], 'the': ['t', 'h', 'e', '</w>'], 'first': ['f', 'i', 'r', 's', 't', '</w>'], 'sentence': ['s', 'e', 'n', 't', 'e', 'n', 'c', 'e', '</w>'], '.': ['.'], 'document': ['d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'], 'second': ['s', 'e', 'c', 'o', 'n', 'd', '</w>'], 'And': ['A', 'n', 'd', '</w>'], 'this': ['t', 'h', 'i', 's', '</w>'], 'third': ['t', 'h', 'i', 'r', 'd', '</w>'], 'one': ['o', 'n', 'e', '</w>'], 'Is': ['I', 's', '</w>'], '?': ['?']}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_freqs = Counter(all_pre_tokens)\n",
    "splits = {word: list(word) + ['</w>'] if word.isalnum() else list(word) for word in word_freqs.keys()}\n",
    "print(\"Word Frequencies:\", word_freqs)\n",
    "print(\"Initial Splits:\", splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Pair Stats (using </w>): Counter({('s', '</w>'): 8, ('i', 's'): 7, ('t', 'h'): 7, ('e', '</w>'): 6, ('h', 'i'): 5, ('t', '</w>'): 5, ('e', 'n'): 5, ('h', 'e'): 4, ('n', 't'): 4, ('i', 'r'): 3, ('d', 'o'): 3, ('o', 'c'): 3, ('c', 'u'): 3, ('u', 'm'): 3, ('m', 'e'): 3, ('d', '</w>'): 3, ('T', 'h'): 2, ('f', 'i'): 2, ('r', 's'): 2, ('s', 't'): 2, ('s', 'e'): 2, ('o', 'n'): 2, ('n', 'd'): 2, ('t', 'e'): 1, ('n', 'c'): 1, ('c', 'e'): 1, ('e', 'c'): 1, ('c', 'o'): 1, ('A', 'n'): 1, ('r', 'd'): 1, ('n', 'e'): 1, ('I', 's'): 1})\n",
      "Most Frequent Pair: (('s', '</w>'), 8)\n"
     ]
    }
   ],
   "source": [
    "def get_pair_stats(splits, word_freqs):\n",
    "    \"\"\"Counts occurrences of adjacent symbol pairs.\"\"\"\n",
    "    stats = Counter()\n",
    "    for word, freq in word_freqs.items():\n",
    "        symbols = splits[word]\n",
    "        if len(symbols) < 2:\n",
    "            continue\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pair = (symbols[i], symbols[i+1])\n",
    "            stats[pair] += freq\n",
    "    return stats\n",
    "\n",
    "initial_pair_stats = get_pair_stats(splits, word_freqs)\n",
    "print(\"\\nInitial Pair Stats (using </w>):\", initial_pair_stats)\n",
    "\n",
    "if initial_pair_stats: \n",
    "    most_frequent_pair = initial_pair_stats.most_common(1)[0] \n",
    "    print(\"Most Frequent Pair:\", most_frequent_pair)\n",
    "else:\n",
    "    print(\"No pairs found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_merges = 100\n",
    "merges = {}\n",
    "current_splits = splits.copy()\n",
    "vocab = initial_vocab.copy()\n",
    "for i in range(num_merges):\n",
    "    pair_stats = get_pair_stats(current_splits, word_freqs)\n",
    "    if not pair_stats:\n",
    "        break\n",
    "    most_frequent_pair = max(pair_stats, key=pair_stats.get)\n",
    "    freq = pair_stats[most_frequent_pair]\n",
    "    if freq < 2:\n",
    "        break\n",
    "    new_symbol = ''.join(most_frequent_pair)\n",
    "    merges[most_frequent_pair] = new_symbol\n",
    "    vocab.add(new_symbol)\n",
    "    new_splits = {}\n",
    "    for word, symbols in current_splits.items():\n",
    "        new_symbols = []\n",
    "        i = 0\n",
    "        while i < len(symbols):\n",
    "            if i < len(symbols) - 1 and (symbols[i], symbols[i+1]) == most_frequent_pair:\n",
    "                new_symbols.append(new_symbol)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_symbols.append(symbols[i])\n",
    "                i += 1\n",
    "        new_splits[word] = new_symbols\n",
    "    current_splits = new_splits\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Vocabulary after Merges: ['document</w>', 'first</w>', 'This</w>', 'this</w>', 'the</w>', 'documen', 'is</w>', 's</w>', 'e</w>', 'docum', '<unk>', 't</w>', 'd</w>', '</w>', 'firs', 'docu', 'doc', 'fir', 'do', 'en', 'th', 'on', 'Th', 'ir', 's', 't', 'o', 'r', 'e', 'm', 'f', 'u', 'c', 'd', 'i', 'T', 'A', 'I', '?', 'h', 'n', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Vocabulary after Merges:\", sorted(list(vocab),key=lambda x: -len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(text, pretokenizer, merges):\n",
    "    pretokens = pretokenizer(text)\n",
    "    result = []\n",
    "    symbols = []\n",
    "    for token in pretokens:\n",
    "        if token.isalnum():\n",
    "            symbols.append((list(token) + ['</w>']))\n",
    "        else:\n",
    "            symbols.append(list(token))\n",
    "    merged_in_pass = True\n",
    "    while merged_in_pass:\n",
    "        merged_in_pass = False\n",
    "        for i in range(len(symbols)):\n",
    "            for j in range(len(symbols[i])-1):\n",
    "                pair = (symbols[i][j], symbols[i][j + 1])\n",
    "                if pair in merges:\n",
    "                    merged_in_pass = True\n",
    "                    new_symbol = merges[pair]\n",
    "                    symbols[i][j:j + 2] = [new_symbol]\n",
    "                    break\n",
    "    result.extend(symbols)\n",
    "    result = sum(result, [])\n",
    "    return result\n",
    "\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: 'This is a first test document.'\n",
      "BPE Tokens: ['This</w>', 'is</w>', 'a', '</w>', 'first</w>', 't', 'e', 's', 't</w>', 'document</w>', '.']\n",
      "\n",
      "Original Text: 'Lowest common documents.'\n",
      "BPE Tokens: ['L', 'o', 'w', 'e', 's', 't</w>', 'c', 'o', 'm', 'm', 'on', '</w>', 'documen', 't', 's</w>', '.']\n"
     ]
    }
   ],
   "source": [
    "new_text = \"This is a first test document.\"\n",
    "bpe_tokens = tokenize(new_text, pretokenize, merges)\n",
    "\n",
    "print(f\"Original Text: '{new_text}'\")\n",
    "print(f\"BPE Tokens: {bpe_tokens}\")\n",
    "\n",
    "another_text = \"Lowest common documents.\"\n",
    "bpe_tokens_unknown = tokenize(another_text, pretokenize, merges)\n",
    "print(f\"\\nOriginal Text: '{another_text}'\")\n",
    "print(f\"BPE Tokens: {bpe_tokens_unknown}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary Dictionary:  {'s': 0, 't': 1, 'o': 2, 'r': 3, 'e': 4, 'm': 5, 'f': 6, 'u': 7, 'c': 8, 'd': 9, 'i': 10, 'T': 11, 'A': 12, 'I': 13, '?': 14, 'h': 15, 'n': 16, '.': 17, 'do': 18, 'en': 19, 'th': 20, 'on': 21, 'Th': 22, 'ir': 23, 'doc': 24, 'fir': 25, '</w>': 26, 'firs': 27, 'docu': 28, 's</w>': 29, 'e</w>': 30, 'docum': 31, '<unk>': 32, 't</w>': 33, 'd</w>': 34, 'is</w>': 35, 'the</w>': 36, 'documen': 37, 'This</w>': 38, 'this</w>': 39, 'first</w>': 40, 'document</w>': 41}\n"
     ]
    }
   ],
   "source": [
    "# vocab to dictionary\n",
    "vocab_dict = {}\n",
    "i = 0\n",
    "for token in sorted(vocab, key=lambda x: len(x)):\n",
    "    if token not in vocab_dict:\n",
    "        vocab_dict[token] = i\n",
    "        i += 1\n",
    "print(\"\\nVocabulary Dictionary: \", vocab_dict)\n",
    "# Save the vocabulary to a file\n",
    "with open('vocab.txt', 'w') as f:\n",
    "    for token, index in vocab_dict.items():\n",
    "        f.write(f\"{token}\\t{index}\\n\")\n",
    "# Save the merges to a file\n",
    "with open('merges.txt', 'w') as f:\n",
    "    for pair, new_symbol in merges.items():\n",
    "        f.write(f\"{pair[0]} {pair[1]} -> {new_symbol}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab_and_merges(vocab_file, merges_file):\n",
    "    vocab_dict = {}\n",
    "    with open(vocab_file, 'r') as f:\n",
    "        for line in f:\n",
    "            token, index = line.strip().split('\\t')\n",
    "            vocab_dict[token] = int(index)\n",
    "    merges_dict = {}\n",
    "    with open(merges_file, 'r') as f:\n",
    "        for line in f:\n",
    "            pair, new_symbol = line.strip().split(' -> ')\n",
    "            merges_dict[tuple(pair.split())] = new_symbol\n",
    "    return vocab_dict, merges_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded Vocabulary Dictionary:  {'s': 0, 't': 1, 'o': 2, 'r': 3, 'e': 4, 'm': 5, 'f': 6, 'u': 7, 'c': 8, 'd': 9, 'i': 10, 'T': 11, 'A': 12, 'I': 13, '?': 14, 'h': 15, 'n': 16, '.': 17, 'do': 18, 'en': 19, 'th': 20, 'on': 21, 'Th': 22, 'ir': 23, 'doc': 24, 'fir': 25, '</w>': 26, 'firs': 27, 'docu': 28, 's</w>': 29, 'e</w>': 30, 'docum': 31, '<unk>': 32, 't</w>': 33, 'd</w>': 34, 'is</w>': 35, 'the</w>': 36, 'documen': 37, 'This</w>': 38, 'this</w>': 39, 'first</w>': 40, 'document</w>': 41}\n"
     ]
    }
   ],
   "source": [
    "vocab_dict, merges_dict = load_vocab_and_merges('vocab.txt', 'merges.txt')\n",
    "print(\"\\nLoaded Vocabulary Dictionary: \", vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text: ['This</w>', 'is</w>', 'a', '</w>', 't', 'e', 's', 't</w>', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenize(\"This is a test.\", pretokenize, merges_dict)\n",
    "print(\"Tokenized Text:\", tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, vocab_dict):\n",
    "    \"\"\"Encodes the text into indices based on the vocabulary dictionary.\"\"\"\n",
    "    tokens = tokenize(text, pretokenize, merges_dict)\n",
    "    encoded = [vocab_dict[token] if token in vocab_dict else vocab_dict[\"<unk>\"] for token in tokens]\n",
    "    return encoded\n",
    "def decode(encoded, vocab_dict):\n",
    "    \"\"\"Decodes the indices back into text.\"\"\"\n",
    "    reverse_vocab = {index: token for token, index in vocab_dict.items()}\n",
    "    decoded = [reverse_vocab[index] if index in reverse_vocab else \"<unk>\" for index in encoded]\n",
    "    return ''.join(decoded).replace('</w>', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Text: [38, 35, 32, 26, 1, 4, 0, 33, 17]\n",
      "Decoded Text: This is <unk> test .\n"
     ]
    }
   ],
   "source": [
    "encoded_text = encode(\"This is a test.\", vocab_dict)\n",
    "print(\"Encoded Text:\", encoded_text)\n",
    "decoded_text = decode(encoded_text, vocab_dict)\n",
    "print(\"Decoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
