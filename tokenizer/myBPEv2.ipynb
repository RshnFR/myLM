{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize(text):\n",
    "    \"\"\"\n",
    "    Pretokenize the input text by splitting it into words and punctuation.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    # Define a regex pattern to match words and punctuation\n",
    "    pattern = r\"\\w+|[^\\w\\s]\"\n",
    "    # Find all matches in the input text\n",
    "    tokens = re.findall(pattern, text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '!', 'This', 'is', 'a', 'test', '.', '.', '.', 'Isn', \"'\", 't', 'it', '?']\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Hello world! This is a test... Isn't it?\"\n",
    "pre_tokens = pretokenize(test_text)\n",
    "print(pre_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-tokens: ['This', 'is', 'the', 'first', 'sentence', '.', 'This', 'document', 'is', 'the', 'second', 'document', '.', 'And', 'this', 'is', 'the', 'third', 'one', '.', 'Is', 'this', 'the', 'first', 'document', '?']\n",
      "Initial Character Vocabulary: ['.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u']\n"
     ]
    }
   ],
   "source": [
    "# Sample Corpus \n",
    "corpus = [\n",
    "  \"This is the first sentence.\",\n",
    "  \"This document is the second document.\",\n",
    "  \"And this is the third one.\",\n",
    "  \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "all_pre_tokens = []\n",
    "for sentence in corpus:\n",
    "  all_pre_tokens.extend(pretokenize(sentence)) \n",
    "print(\"Pre-tokens:\", all_pre_tokens)\n",
    "\n",
    "initial_vocab = set()\n",
    "for token in all_pre_tokens:\n",
    "  initial_vocab.update(list(token))\n",
    "\n",
    "print(\"Initial Character Vocabulary:\", sorted(list(initial_vocab)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Frequencies: Counter({'the': 4, 'is': 3, '.': 3, 'document': 3, 'This': 2, 'first': 2, 'this': 2, 'sentence': 1, 'second': 1, 'And': 1, 'third': 1, 'one': 1, 'Is': 1, '?': 1})\n",
      "Initial Splits: {'This': ['T', 'h', 'i', 's', '</w>'], 'is': ['i', 's', '</w>'], 'the': ['t', 'h', 'e', '</w>'], 'first': ['f', 'i', 'r', 's', 't', '</w>'], 'sentence': ['s', 'e', 'n', 't', 'e', 'n', 'c', 'e', '</w>'], '.': ['.'], 'document': ['d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'], 'second': ['s', 'e', 'c', 'o', 'n', 'd', '</w>'], 'And': ['A', 'n', 'd', '</w>'], 'this': ['t', 'h', 'i', 's', '</w>'], 'third': ['t', 'h', 'i', 'r', 'd', '</w>'], 'one': ['o', 'n', 'e', '</w>'], 'Is': ['I', 's', '</w>'], '?': ['?']}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_freqs = Counter(all_pre_tokens)\n",
    "splits = {word: list(word) + ['</w>'] if word.isalnum() else list(word) for word in word_freqs.keys()}\n",
    "print(\"Word Frequencies:\", word_freqs)\n",
    "print(\"Initial Splits:\", splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Pair Stats (using </w>): Counter({('s', '</w>'): 8, ('i', 's'): 7, ('t', 'h'): 7, ('e', '</w>'): 6, ('h', 'i'): 5, ('t', '</w>'): 5, ('e', 'n'): 5, ('h', 'e'): 4, ('n', 't'): 4, ('i', 'r'): 3, ('d', 'o'): 3, ('o', 'c'): 3, ('c', 'u'): 3, ('u', 'm'): 3, ('m', 'e'): 3, ('d', '</w>'): 3, ('T', 'h'): 2, ('f', 'i'): 2, ('r', 's'): 2, ('s', 't'): 2, ('s', 'e'): 2, ('o', 'n'): 2, ('n', 'd'): 2, ('t', 'e'): 1, ('n', 'c'): 1, ('c', 'e'): 1, ('e', 'c'): 1, ('c', 'o'): 1, ('A', 'n'): 1, ('r', 'd'): 1, ('n', 'e'): 1, ('I', 's'): 1})\n",
      "Most Frequent Pair: (('s', '</w>'), 8)\n"
     ]
    }
   ],
   "source": [
    "def get_pair_stats(splits, word_freqs):\n",
    "    \"\"\"Counts occurrences of adjacent symbol pairs.\"\"\"\n",
    "    stats = Counter()\n",
    "    for word, freq in word_freqs.items():\n",
    "        symbols = splits[word]\n",
    "        if len(symbols) < 2:\n",
    "            continue\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pair = (symbols[i], symbols[i+1])\n",
    "            stats[pair] += freq\n",
    "    return stats\n",
    "\n",
    "initial_pair_stats = get_pair_stats(splits, word_freqs)\n",
    "print(\"\\nInitial Pair Stats (using </w>):\", initial_pair_stats)\n",
    "\n",
    "if initial_pair_stats: \n",
    "    most_frequent_pair = initial_pair_stats.most_common(1)[0] \n",
    "    print(\"Most Frequent Pair:\", most_frequent_pair)\n",
    "else:\n",
    "    print(\"No pairs found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_merges = 100\n",
    "merges = {}\n",
    "current_splits = splits.copy()\n",
    "vocab = initial_vocab.copy()\n",
    "for i in range(num_merges):\n",
    "    pair_stats = get_pair_stats(current_splits, word_freqs)\n",
    "    if not pair_stats:\n",
    "        break\n",
    "    most_frequent_pair = max(pair_stats, key=pair_stats.get)\n",
    "    freq = pair_stats[most_frequent_pair]\n",
    "    if freq < 2:\n",
    "        break\n",
    "    new_symbol = ''.join(most_frequent_pair)\n",
    "    merges[most_frequent_pair] = new_symbol\n",
    "    vocab.add(new_symbol)\n",
    "    new_splits = {}\n",
    "    for word, symbols in current_splits.items():\n",
    "        new_symbols = []\n",
    "        i = 0\n",
    "        while i < len(symbols):\n",
    "            if i < len(symbols) - 1 and (symbols[i], symbols[i+1]) == most_frequent_pair:\n",
    "                new_symbols.append(new_symbol)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_symbols.append(symbols[i])\n",
    "                i += 1\n",
    "        new_splits[word] = new_symbols\n",
    "    current_splits = new_splits\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Vocabulary after Merges: ['document</w>', 'first</w>', 'This</w>', 'this</w>', 'the</w>', 'documen', 'is</w>', 's</w>', 'e</w>', 'docum', 't</w>', 'd</w>', 'firs', 'docu', 'doc', 'fir', 'do', 'en', 'th', 'on', 'Th', 'ir', 's', 't', 'o', 'r', 'e', 'm', 'f', 'u', 'c', 'd', 'i', 'T', 'A', 'I', '?', 'h', 'n', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Vocabulary after Merges:\", sorted(list(vocab),key=lambda x: -len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, pretokenizer, merges):\n",
    "    pretokens = pretokenizer(text)\n",
    "    result = []\n",
    "    symbols = []\n",
    "    for token in pretokens:\n",
    "        if token.isalnum():\n",
    "            symbols.append((list(token) + ['</w>']))\n",
    "        else:\n",
    "            symbols.append(list(token))\n",
    "    merged_in_pass = True\n",
    "    while merged_in_pass:\n",
    "        merged_in_pass = False\n",
    "        for i in range(len(symbols)):\n",
    "            for j in range(len(symbols[i])-1):\n",
    "                pair = (symbols[i][j], symbols[i][j + 1])\n",
    "                if pair in merges:\n",
    "                    merged_in_pass = True\n",
    "                    new_symbol = merges[pair]\n",
    "                    symbols[i][j:j + 2] = [new_symbol]\n",
    "                    break\n",
    "    result.extend(symbols)\n",
    "    return result\n",
    "\n",
    "\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: 'This is a first test document.'\n",
      "BPE Tokens: [['This</w>'], ['is</w>'], ['a', '</w>'], ['first</w>'], ['t', 'e', 's', 't</w>'], ['document</w>'], ['.']]\n",
      "\n",
      "Original Text: 'Lowest common documents.'\n",
      "BPE Tokens: [['L', 'o', 'w', 'e', 's', 't</w>'], ['c', 'o', 'm', 'm', 'on', '</w>'], ['documen', 't', 's</w>'], ['.']]\n"
     ]
    }
   ],
   "source": [
    "new_text = \"This is a first test document.\"\n",
    "bpe_tokens = tokenize(new_text, pretokenize, merges)\n",
    "\n",
    "print(f\"Original Text: '{new_text}'\")\n",
    "print(f\"BPE Tokens: {bpe_tokens}\")\n",
    "\n",
    "another_text = \"Lowest common documents.\"\n",
    "bpe_tokens_unknown = tokenize(another_text, pretokenize, merges)\n",
    "print(f\"\\nOriginal Text: '{another_text}'\")\n",
    "print(f\"BPE Tokens: {bpe_tokens_unknown}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
